{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a deep denoising autoencoder\n",
    "## Version 1: Getting the autoencoder started; compares two sets of hyperparameters.\n",
    "Reference: https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, intermediate_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(\n",
    "            units=intermediate_dim,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer='he_uniform'\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=intermediate_dim,\n",
    "            activation=tf.nn.sigmoid\n",
    "        )\n",
    "\n",
    "    def call(self, input_features):\n",
    "        activation = self.hidden_layer(input_features)\n",
    "        return self.output_layer(activation)\n",
    "\n",
    "\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(\n",
    "            units=intermediate_dim,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer='he_uniform'\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=original_dim,\n",
    "            activation=tf.nn.sigmoid\n",
    "        )\n",
    "  \n",
    "    def call(self, code):\n",
    "        activation = self.hidden_layer(code)\n",
    "        return self.output_layer(activation)\n",
    "    \n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "        self.next_encoder = Encoder(intermediate_dim = intermediate_dim)\n",
    "        self.next_decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "\n",
    "        \n",
    "    def call(self, input_features):\n",
    "        code = self.encoder(input_features)\n",
    "        reconstructed = self.decoder(code)\n",
    "        re_code = self.next_encoder(reconstructed)\n",
    "        re_reconstructed = self.next_decoder(re_code)\n",
    "        return re_reconstructed\n",
    "    \n",
    "\n",
    "def loss(model, original):\n",
    "    reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
    "    return reconstruction_error\n",
    "\n",
    "\n",
    "def train(loss, model, opt, original):\n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients = tape.gradient(loss(model, original), model.trainable_variables)\n",
    "        gradient_variables = zip(gradients, model.trainable_variables)\n",
    "        opt.apply_gradients(gradient_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import data, constant\n",
    "from tensorflow.summary import create_file_writer, record_if, scalar, image\n",
    "from tensorflow import reshape\n",
    "from tensorflow import optimizers\n",
    "\n",
    "# Build a progress bar\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2\n",
    "epochs = 10\n",
    "\n",
    "(training_features, _), (test_features, _) = mnist.load_data()\n",
    "num_training = len(training_features)\n",
    "training_features = training_features / np.max(training_features)\n",
    "training_features = training_features.reshape(training_features.shape[0],\n",
    "                                              training_features.shape[1] * training_features.shape[2])\n",
    "training_features = training_features.astype('float32')\n",
    "training_dataset = data.Dataset.from_tensor_slices(training_features)\n",
    "training_dataset = training_dataset.batch(batch_size)\n",
    "training_dataset = training_dataset.shuffle(training_features.shape[0])\n",
    "training_dataset = training_dataset.prefetch(batch_size * 4)\n",
    "\n",
    "progbar = Progbar(epochs*num_training/batch_size)\n",
    "\n",
    "#autoencoder1 = Autoencoder(intermediate_dim=64, original_dim=784)\n",
    "autoencoder1 = Autoencoder(intermediate_dim=42, original_dim=784)\n",
    "autoencoder2 = Autoencoder(intermediate_dim=30, original_dim=784)\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "writer = create_file_writer('tmp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block compares the output (reconstructed) images and losses of the two autoencoders defined above. The results can be viewed in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with writer.as_default():\n",
    "    with record_if(True):\n",
    "        for epoch in range(epochs):\n",
    "            for step, batch_features in enumerate(training_dataset):\n",
    "                train(loss, autoencoder1, opt, batch_features)\n",
    "                train(loss, autoencoder2, opt, batch_features)\n",
    "                loss_values1 = loss(autoencoder1, batch_features)\n",
    "                loss_values2 = loss(autoencoder2, batch_features)\n",
    "                original = reshape(batch_features, (batch_features.shape[0], 28, 28, 1))\n",
    "                reconstructed1 = reshape(autoencoder1(constant(batch_features)), (batch_features.shape[0], 28, 28, 1))\n",
    "                reconstructed2 = reshape(autoencoder2(constant(batch_features)), (batch_features.shape[0], 28, 28, 1))\n",
    "                scalar('loss1', loss_values1, step=step)\n",
    "                scalar('loss2', loss_values2, step=step)\n",
    "                image('original', original, max_outputs=10, step=step)\n",
    "                image('reconstructed1', reconstructed1, max_outputs=10, step=step)\n",
    "                image('reconstructed2', reconstructed2, max_outputs=10, step=step)\n",
    "                progbar.update(1+step*(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2: Uses KerasTuner to find better hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hypermodel import HyperModel\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "(X, y), (X_val, y_val) = keras.datasets.mnist.load_data()\n",
    "X = X.astype('float32')/255.\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "X_val = X_val.astype('float32')/255.\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1]*X_val.shape[2])\n",
    "\n",
    "\n",
    "X_train, y_train = X[:10000], y[:10000]\n",
    "X_test, y_test = X[10000:], y[10000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, intermediate_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(\n",
    "            units=intermediate_dim,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer='he_uniform'\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=intermediate_dim,\n",
    "            activation=tf.nn.sigmoid\n",
    "        )\n",
    "\n",
    "    def call(self, input_features):\n",
    "        activation = self.hidden_layer(input_features)\n",
    "        return self.output_layer(activation)\n",
    "\n",
    "\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(\n",
    "            units=intermediate_dim,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer='he_uniform'\n",
    "        )\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=original_dim,\n",
    "            activation=tf.nn.sigmoid\n",
    "        )\n",
    "  \n",
    "    def call(self, code):\n",
    "        activation = self.hidden_layer(code)\n",
    "        return self.output_layer(activation)\n",
    "    \n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, intermediate_dim, original_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "        self.next_encoder = Encoder(intermediate_dim = intermediate_dim)\n",
    "        self.next_decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "\n",
    "        \n",
    "    def call(self, input_features):\n",
    "        code = self.encoder(input_features)\n",
    "        reconstructed = self.decoder(code)\n",
    "        re_code = self.next_encoder(reconstructed)\n",
    "        re_reconstructed = self.next_decoder(re_code)\n",
    "        return re_reconstructed\n",
    "\n",
    "def loss(model, original):\n",
    "    reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
    "    return reconstruction_error\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Autoencoder(intermediate_dim = hp.Int('int_dim', 12, 72, 12), original_dim = 784)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-3, 1e-4])),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses KerasTuner's RandomSearch to find better hyperparameters for the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: this takes a while to run ~30min\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=3,\n",
    "    directory='test',\n",
    "    overwrite=True)\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(X_train,\n",
    "             X_train,\n",
    "             epochs=10,\n",
    "             verbose = 3,\n",
    "             validation_data=(X_val, X_val))\n",
    "\n",
    "tuner.results_summary()\n",
    "\n",
    "# lr = .0001 = 1e-3\n",
    "# dim = 48\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chooses the best model, and shows some original/reconstructed image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.fit(X_train, X_train)\n",
    "best_model.summary()\n",
    "best_model.save('autoencoder')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "originals = X_test[:10]\n",
    "new = best_model.predict(originals)\n",
    "\n",
    "for i in range(len(originals)):\n",
    "    im1 = originals[i]\n",
    "    nim1 = new[i]\n",
    "\n",
    "    im1 = im1.reshape(28,28)\n",
    "    nim1 = nim1.reshape(28,28)\n",
    "\n",
    "    plt.imshow(im1, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(nim1, cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
